{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Generation with Transformers Pre Screening\n",
    "\n",
    "This following project is an example Transformer built with PyTorch trained on the Yelp dataset. It takes in Yelp user reviews and attempts to figure out how high the rating a business might have had based on the user reviews. This transformer parses through user text and then gives the predicted rating of the user. The user can also input their own statements and then have the transformer attempt to guess how high of a rating they would give based on the text they've written."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the following can be downloaded by using `env.yml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import math\n",
    "import torch\n",
    "import datasets\n",
    "import statistics\n",
    "import numpy as np\n",
    "import tarfile\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.corpus import brown\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TA_CACHE_DIR'] = 'data/'\n",
    "os.environ['NLTK_DATA'] = 'nltk_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the seed\n",
    "SEED = 2002\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "print(f'Seed {SEED} has been set.')\n",
    "\n",
    "# Setting up CUDA and using GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device != \"cuda\":\n",
    "  print(\"Not using GPU.\")\n",
    "else:\n",
    "  print(\"GPU is enabled in this notebook.\")\n",
    "DEVICE = device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Set up and Helper Functions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Data\n",
    "fname = 'nltk_data.zip'\n",
    "url = 'https://osf.io/download/zqw5s/'\n",
    "\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "\n",
    "with open(fname, 'wb') as fd:\n",
    "  fd.write(r.content)\n",
    "\n",
    "with zipfile.ZipFile(fname, 'r') as zip_ref:\n",
    "  zip_ref.extractall('.')\n",
    "\n",
    "# Hugging Face Yelp Data\n",
    "## The Yelp dataset contains Yelp reviews and businesses. With this dataset, we can look into how positive the customer's reviews may be towards specific places.\n",
    "fname = \"huggingface.tar.gz\"\n",
    "url = \"https://osf.io/kthjg/download\"\n",
    "\n",
    "if not os.path.exists(fname):\n",
    "  print('Downloading dataset')\n",
    "  r = requests.get(url, allow_redirects=True)\n",
    "  with open(fname, 'wb') as fd:\n",
    "    fd.write(r.content)\n",
    "  print('Download finished.')\n",
    "  with tarfile.open(fname) as ft:\n",
    "    ft.extractall('data/')\n",
    "  print('Files have been extracted.')\n",
    "\n",
    "if os.path.exists(fname):\n",
    "  print(\"It is already there! Loading data now.\")\n",
    "  DATASET = datasets.load_dataset(\"yelp_review_full\", download_mode=\"reuse_dataset_if_exists\", cache_dir='data/')\n",
    "\n",
    "#This part just checks everything is running perfectly\n",
    "print(type(DATASET))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = ['editorial', 'fiction', 'government', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
    "brown_wordlist = list(brown.words(categories=category))\n",
    "\n",
    "def create_word2vec_model(category = 'news', size = 50, sg = 1, min_count = 10):\n",
    "    sentences = brown.sents(categories=category)\n",
    "    model = Word2Vec(sentences, vector_size=size, sg=sg, min_count=min_count)\n",
    "    return model\n",
    "\n",
    "w2vmodel = create_word2vec_model(category)\n",
    "\n",
    "def model_dictionary():\n",
    "  print(w2vmodel.wv)\n",
    "  return list(w2vmodel.wv)\n",
    "\n",
    "def get_embedding(word, model):\n",
    "  try:\n",
    "    return model.wv[word]\n",
    "  except KeyError:\n",
    "    print(f' |{word}| not in model dictionary. Try another word')\n",
    "\n",
    "def check_word_in_corpus(word, model):\n",
    "  try:\n",
    "    word_embedding = model.wv[word]\n",
    "    print('Word present!')\n",
    "    return word_embedding\n",
    "  except KeyError:\n",
    "    print('Word NOT present!')\n",
    "    return None\n",
    "\n",
    "def get_embeddings(words,model):\n",
    "  embed_list = [get_embedding(word,model) for word in words]\n",
    "  return np.array(embed_list)\n",
    "\n",
    "def softmax(x):\n",
    "  return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "def transform_sentence_for_bert(sent, masked_word = \"___\"):\n",
    "  splitted = sent.split(\"___\")\n",
    "  assert (len(splitted) == 2), \"Missing masked word. Make sure to mark it as ___\"\n",
    "  return '[CLS] ' + splitted[0] + \"[MASK]\" + splitted[1] + ' [SEP]'\n",
    "\n",
    "\n",
    "def parse_text_and_words(raw_line, mask = \"___\"):\n",
    "  splitted = raw_line.split(' ')\n",
    "  mask_index = -1\n",
    "  for i in range(len(splitted)):\n",
    "    if \"/\" in splitted[i]:\n",
    "      mask_index = i\n",
    "      break\n",
    "  assert(mask_index != -1), \"No '/'-separated words\"\n",
    "  words = splitted[mask_index].split('/')\n",
    "  splitted[mask_index] = mask\n",
    "  return \" \".join(splitted), words\n",
    "\n",
    "\n",
    "def get_probabilities_of_masked_words(text, words):\n",
    "  text = transform_sentence_for_bert(text)\n",
    "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "  for i in range(len(words)):\n",
    "    words[i] = tokenizer.tokenize(words[i])[0]\n",
    "  words_idx = [tokenizer.convert_tokens_to_ids([word]) for word in words]\n",
    "  tokenized_text = tokenizer.tokenize(text)\n",
    "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "  masked_index = tokenized_text.index('[MASK]')\n",
    "  tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "  pretrained_masked_model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "  pretrained_masked_model.eval()\n",
    "\n",
    "  with torch.no_grad():\n",
    "    predictions = pretrained_masked_model(tokens_tensor)\n",
    "  probabilities = F.softmax(predictions[0][masked_index], dim = 0)\n",
    "\n",
    "  return [probabilities[ix].item() for ix in words_idx]\n",
    "\n",
    "def load_yelp_data(DATASET, tokenizer):\n",
    "  dataset = DATASET\n",
    "  dataset['train'] = dataset['train'].select(range(10000))\n",
    "  dataset['test'] = dataset['test'].select(range(5000))\n",
    "  dataset = dataset.map(lambda e: tokenizer(e['text'], truncation=True,\n",
    "                                                padding='max_length'), batched=True)\n",
    "  dataset.set_format(type='torch', columns=['input_ids', 'label'])\n",
    "\n",
    "  train_loader = torch.utils.data.DataLoader(dataset['train'], batch_size=32)\n",
    "  test_loader = torch.utils.data.DataLoader(dataset['test'], batch_size=32)\n",
    "\n",
    "  vocab_size = tokenizer.vocab_size\n",
    "  max_len = next(iter(train_loader))['input_ids'].shape[0]\n",
    "  num_classes = next(iter(train_loader))['label'].shape[0]\n",
    "\n",
    "  return train_loader, test_loader, max_len, vocab_size, num_classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "Tokenizers prepare inputs for the model through encoding and decoding inputs into something a computer system can understand. The following follows the tokenization system BERT utilizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased', cache_dir='data/')\n",
    "train_loader, test_loader, max_len, vocab_size, num_classes = load_yelp_data(DATASET, tokenizer)\n",
    "\n",
    "pred_text = DATASET['test']['text'][28]\n",
    "actual_label = DATASET['test']['label'][28]\n",
    "batch1 = next(iter(test_loader))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beginning with Attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start off by creating a function that allows us to compute the scaled dot producted attention. We'll be applying these layers instead of `torch.nn.Transformer()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(nn.Module):\n",
    "  def __init__(self, dropout, **kwargs):\n",
    "    super(DotProductAttention, self).__init__(**kwargs)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def calculate_score(self, queries, keys):\n",
    "    return torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(queries.shape[-1])\n",
    "\n",
    "  def forward(self,queries,keys,values,b,h,t,k):\n",
    "    keys = keys.transpose(1, 2).contiguous().view(b * h,t,k)\n",
    "    queries = queries.transpose(1, 2).contiguous().view(b * h,t,k)\n",
    "    values = values.transpose(1, 2).contiguous().view(b * h,t,k)\n",
    "\n",
    "    score = self.calculate_score(queries, keys)\n",
    "    softmax_weights = F.softmax(score, dim=2)\n",
    "\n",
    "    output = torch.bmm(self.dropout(softmax_weights), values).view(b,h,t,k)\n",
    "    out = output.transpose(1, 2).contiguous().view(b,t, h * k)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also be creating a multi-head self attention layer which captures different aspects of the dependence amongst words. This mechanism runs through the scaled dot-product attention multiple times in parallel and then the ouputs are concatenated and then linearly transformed into expected dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "  def __init__(self, k, heads=8, dropout=0.1):\n",
    "    super().__init__()\n",
    "    self.k, self.heads = k, heads\n",
    "\n",
    "    self.to_keys = nn.Linear(k,k * heads,bias=False)\n",
    "    self.to_queries = nn.Linear(k,k * heads,bias=False)\n",
    "    self.to_values = nn.Linear(k,k * heads,bias=False)\n",
    "    self.unify_heads = nn.Linear(k * heads,k)\n",
    "\n",
    "    self.attention = DotProductAttention(dropout)\n",
    "\n",
    "  def forward(self,x):\n",
    "    b, t, k = x.size()\n",
    "    h = self.heads\n",
    "    queries = self.to_queries(x).view(b,t,h,k)\n",
    "    keys = self.to_keys(x).view(b,t,h,k)\n",
    "    values = self.to_values(x).view(b,t,h,k)\n",
    "    out = self.attention(queries,keys,values, b, h, t, k)\n",
    "\n",
    "    return self.unify_heads(out)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "  #Source: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "  def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "    super().__init__()\n",
    "    self.dropout = nn.Dropout(p=dropout)\n",
    "    position = torch.arange(max_len).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "    pe = torch.zeros(max_len, 1, d_model)\n",
    "    pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "    self.register_buffer('pe', pe)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x + self.pe[:x.size(0)]\n",
    "    return self.dropout(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer blocks are made up of self attention, layer normalization and feedfoward neural networks. The following showcases what we'd expect a transformer block to look like. While in reality, we could just use `torch.nn.Transformer()`, this provides us a deeper idea of how exactly the transformer layer actually works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "  def __init__(self, k, heads):\n",
    "    super().__init__()\n",
    "\n",
    "    self.attention = SelfAttention(k, heads=heads)\n",
    "    self.norm_1 = nn.LayerNorm(k)\n",
    "    self.norm_2 = nn.LayerNorm(k)\n",
    "\n",
    "    hidden_size = 2 * k\n",
    "    self.mlp = nn.Sequential(nn.Linear(k, hidden_size), nn.ReLU(), nn.Linear(hidden_size, k))\n",
    "\n",
    "  def forward(self, x):\n",
    "    attended = self.attention(x)\n",
    "    x = self.norm_1(attended + x)\n",
    "\n",
    "    feedforward = self.mlp(x)\n",
    "    x = self.norm_2(feedforward + x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also make a positional encoding function that allows us to represent word orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    #Source: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "  def __init__(self, k, heads, depth, seq_length, num_tokens, num_classes):\n",
    "    super().__init__()\n",
    "\n",
    "    self.k = k\n",
    "    self.num_tokens = num_tokens\n",
    "    self.token_embedding = nn.Embedding(num_tokens, k)\n",
    "    self.pos_enc = PositionalEncoding(k)\n",
    "\n",
    "    transformer_blocks = []\n",
    "    for i in range(depth):\n",
    "      transformer_blocks.append(TransformerBlock(k=k, heads=heads))\n",
    "\n",
    "    self.transformer_blocks = nn.Sequential(*transformer_blocks)\n",
    "    self.classification_head = nn.Linear(k, num_classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.token_embedding(x) * np.sqrt(self.k)\n",
    "    x = self.pos_enc(x)\n",
    "    x = self.transformer_blocks(x)\n",
    "\n",
    "    sequence_avg = x.mean(dim=1)\n",
    "    x = self.classification_head(sequence_avg)\n",
    "    logprobs = F.log_softmax(x, dim=1)\n",
    "    return logprobs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_fn, train_loader,\n",
    "          n_iter=1, learning_rate=1e-4,\n",
    "          test_loader=None, device='cpu',\n",
    "          L2_penalty=0, L1_penalty=0):\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "  train_loss = []\n",
    "  test_loss = []\n",
    "\n",
    "  for iter in range(n_iter):\n",
    "    iter_train_loss = []\n",
    "    for i, batch in tqdm(enumerate(train_loader)):\n",
    "      out = model(batch['input_ids'].to(device))\n",
    "      loss = loss_fn(out, batch['label'].to(device))\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      loss.backward()\n",
    "\n",
    "      optimizer.step()\n",
    "\n",
    "      iter_train_loss.append(loss.item())\n",
    "      if i % 50 == 0:\n",
    "        print(f'[Batch {i}]: train_loss: {loss.item()}')\n",
    "    train_loss.append(statistics.mean(iter_train_loss))\n",
    "\n",
    "    if True:\n",
    "      if test_loader is not None:\n",
    "        print('Running Test loop')\n",
    "        iter_loss_test = []\n",
    "        for j, test_batch in enumerate(test_loader):\n",
    "\n",
    "          out_test = model(test_batch['input_ids'].to(device))\n",
    "          loss_test = loss_fn(out_test, test_batch['label'].to(device))\n",
    "          iter_loss_test.append(loss_test.item())\n",
    "\n",
    "        test_loss.append(statistics.mean(iter_loss_test))\n",
    "\n",
    "      if test_loader is None:\n",
    "        print(f'iteration {iter + 1}/{n_iter} | train loss: {loss.item():.3f}')\n",
    "      else:\n",
    "        print(f'iteration {iter + 1}/{n_iter} | train loss: {loss.item():.3f} | test_loss: {loss_test.item():.3f}')\n",
    "\n",
    "  if test_loader is None:\n",
    "    return train_loss\n",
    "  else:\n",
    "    return train_loss, test_loss\n",
    "\n",
    "\n",
    "Transformer_model = Transformer(128, 8, 3, max_len, vocab_size, num_classes).to(DEVICE)\n",
    "\n",
    "loss_fn = F.nll_loss\n",
    "\n",
    "# Make sure you run this on your GPU otherwise it takes a REALY long time\n",
    "if DEVICE != 'cpu':\n",
    "  train_loss, test_loss = train(Transformer_model,loss_fn, train_loader, test_loader=test_loader, device=DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part showcases the predictive capabilities of transformers. Here we send in a review to the transformer and it sends out a predicted label for the rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  pred_batch = Transformer_model(batch1['input_ids'].to(DEVICE))\n",
    "  print(\"The yelp review is: \" + str(pred_text))\n",
    "  predicted_label28 = np.argmax(pred_batch[28].cpu())\n",
    "  print(\"The Predicted Rating is: \" + str(predicted_label28.item()) + \". The actual rating was: \" + str(actual_label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
